# Emma â€“ Local AI Assistant ğŸ¤–

Emma is a modular, offline-capable AI assistant built using local large language models.  
It supports persistent conversation memory, natural dialogue, and offline voice output.

This project focuses on **system design**, not model training.

---

## âœ¨ Features

- ğŸ§  Persistent conversation memory (SQLite)
- ğŸ’¬ Context-aware responses
- ğŸ”’ Offline-first (no cloud dependency)
- ğŸ—£ Voice output using Piper TTS
- âš™ Modular and extensible architecture
- ğŸš€ Lightweight and fast

---

## ğŸ— Architecture

User Input â†’ Assistant Logic â†’ Local LLM (Ollama)
â†’ Response â†’ Text + Voice Output

---

## ğŸ›  Technologies

- Python 3
- Ollama (Local LLM runtime)
- SQLite
- Piper TTS
- SoundDevice / SoundFile

---

## ğŸ“¦ Setup

### 1. Install dependencies
```bash
pip install requirements.txt
```
### 2. Run Ollama
```bash
ollama pull phi3
ollama serve
```
### 3. Run Emma
```bash
python main.py
```

## ğŸ¯ Project Goal

To demonstrate how a local AI assistant system can be built with memory, modularity, and offline capabilities, serving as a foundation for future automation and IoT integration.

## ğŸ”® Future Plans

- Command execution

- Hardware & IoT integration

- Email and task automation

- Speech input

- Cloudâ€“local hybrid model support

## ğŸ“œ License

This project is intended for educational use.


